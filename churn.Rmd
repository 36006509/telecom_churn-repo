---
title: "churn"
output:
  md_document: default
  rmarkdown::github_document: default
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TELECOM CHURN PREDICTION

Okay, today we will try to build some predictive model. To do so, We will be using the Tidymodel framework which is very handy.

The Orange telecom's churn dataset have been uploaded from Kaggle and you can easily access to it by clicking on this [link](https://www.kaggle.com/mnassrib/telecom-churn-datasets "link to the dataset"). But first of all, let's load all the package that we will need for this project.

## Loading all packages

```{r load, message=FALSE, warning=FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(parsnip)
library(yardstick)
library(workflowsets)
library(dplyr)
library(DataExplorer)
library(kknn)
library(themis)
library(glmnet)
library(rpart)
library(readr)
library(kableExtra)
library(klaR)
library(discrim)
library(randomForest)
```

## import our data

As we see below, the Dataset contains 2666 observations and about 20 features like their personal information and their daily usage and purchase. the outcome is the variable "Churn" specifying whether a customer canceled a subscription.

```{r data, message=TRUE, warning=FALSE}
churn<- read.csv("https://raw.githubusercontent.com/36006509/telecom_churn-repo/main/churn-bigml-80.csv",stringsAsFactors = TRUE)
#churn[is.double(churn)]<- as.integer(churn[is.double(churn)])
#churn[is.integer(churn)]<- as.numeric(churn[is.integer(churn)])
churn$Churn<-if_else(churn$Churn=="False","NON","OUI")
churn$Churn<- as.factor(churn$Churn)
str(churn)
```

## Exploratory Data Analysis

### Univaritate analysis

Now, let's see take a look at our features

```{r EDA}
plot_intro(churn)

```

only 20% of our features are nominal and their is no missing observation which is a good thing.

```{r}

plot_boxplot(churn, by = "Churn")

churn %>%
  count(Churn)%>%
  mutate(p=n/sum(n))
```

we can see that almost 86 percent of the clients are not churner. So, it seems like we are facing an imbalanced data. to fix that we need to do some data augmentation.

```{r hist}
churn_num<-bind_cols(
  select_if(churn, is.numeric),
  select_at(churn, "Churn")
)
churn_fact<- select_if(churn,is.factor)
plot_bar(churn_fact)

```

```{}
```

### Bivariate Analysis

Now let's check for some features correlations.

```{r corr}
plot_correlation(churn,type = "d")#correlation between discrete variables
plot_correlation(churn,type="c")#correlation between continuous variables



#GGally::scatmat(churn_num,1:ncol(churn_num),color = "Churn")
```

As we see some features are highly correlated like total.intl.charge and total.intl.minutes. Thus, we will keep only one of them for the modelisation. We'll do the same thing for all correlated features.

## Processing

### training and testing 

```{r split, message=FALSE, warning=FALSE}
set.seed(123)
churn_split<- initial_split(churn,
                             prop = 0.75,
                            strata = Churn)
churn_train<- churn_split %>% training()
churn_test<- churn_split%>% testing()

#folds caracteristics for the cross validation 
set.seed(2)
churn_folds <- vfold_cv(data =  churn_train,
                       #number of partition
                       v = 5,
                       #outcome variable
                       strata = Churn)


```

### Recipe and features Engineering

-   Create recipe by specifying outcome and predictors, we'll be using the train dataset

-   step_relevel: to change the reference level to "OUI"

-   step_normalize: to normalize all our numeric variables

-   step_corr: to drop all numeric predictors that are high correlated

-   step_dummy: create dummy variables for all nominal variables

-   step_smote: to deal with imbalanced data

```{r recipe, message=FALSE, warning=FALSE}


churn_rec<- recipe(Churn ~., data = churn_train) %>% 
  #set the event/reference level to 'good'
  step_relevel(Churn, ref_level = 'OUI') %>% 
  
  #normalize all numeric variables
  step_normalize(all_numeric()) %>% 
  
  step_corr(all_numeric_predictors(),threshold = 0.6)%>%
  
  #turn all the factors into dummies and delete the reference level
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_smote(Churn)
  

```

## Model specification using parsnip and tune

let's now dive into the modelisation. Each model that we 'll be using needs some hyper-parameter tuning. to do so, we will first specify all our models and tune their respective hyper_parameter.

-   Logistic Regression

-   K nearest neighbor

-   Random forest

-   Decision tree

-   Bayesian model

```{r model}

#logistic regression
logit_tuned <- logistic_reg(penalty = tune(), 
                            mixture = tune()) %>%
  set_engine('glmnet') %>%
  set_mode('classification')

#Decision Tree

dt_tuned <- decision_tree(cost_complexity = tune(),
                               tree_depth = tune(),
                               min_n = tune()) %>%
  set_engine('rpart') %>%
  set_mode('classification')

bayes_tuned<- naive_Bayes(smoothness=tune(),
                          Laplace=tune())%>%
  set_engine('klaR')%>%
  set_mode('classification')

rf_tuned <- rand_forest(mtry = tune(),
                        trees = tune(),
                        min_n = tune()) %>% 
  set_engine('randomForest') %>%
  set_mode('classification')

```

## Modeling

### Creating workflowset

let's now create our workflow by:

-   making a list out of the models

-   put these models inside a set of workflow

-   specify the metrics that we will be using for the model evaluation

```{r models}
#make a list out of the models
models <- list(logit = logit_tuned,
               dt = dt_tuned,
               b=bayes_tuned,
               rd=rf_tuned)
#incorporate them in a set of workflow
churn_wflow_set <- workflow_set(preproc = list(rec=churn_rec), 
                               models = models, 
                               cross = TRUE)  
#metrics we want for each model 
#we want : accuracy, sensitivity, specificity, area under the roc curve 
churn_metrics <- metric_set(accuracy, sens, spec, roc_auc)

```

### Model tuning

For each model tuning we will use a cross-validation and the tune_grid() function will choose randomly 10 combinations of the hyper-parameters. we will see later which of those combination is the best.

```{r tuned model}
wflow_set_grid_results <- churn_wflow_set %>% 
  workflow_map(
  #tune_grid() parameters
    resamples = churn_folds,
    grid = 10,
    metrics = churn_metrics,
  #workflow_map() own parameters
    seed = 3,
    verbose = TRUE)
```

```{r best}
wflow_set_grid_results %>% 
  rank_results(rank_metric = "accuracy", select_best = TRUE) %>% 
  filter(.metric == "accuracy" | .metric == "sens" | .metric == "spec" )%>% 
  kbl() %>% 
  kable_styling() %>% 
  scroll_box(width = "100%", height = "200px")

```

```{r plot}
#plot the performance of each model by rank
wflow_set_grid_results %>% 
  autoplot(rank_metric= "roc_auc", 
           metric = "roc_auc")

```

The plot bellow tells us that the random forest performs better than the other models. but it takes a few minutes to run as it may selects about 400 trees. so now, let's pull the best result within the workflow and finalize our model

```{r best metrics}
#take the best result
best_results <- wflow_set_grid_results %>% 
  pull_workflow_set_result("rec_rd") %>% 
  select_best(metric = "roc_auc")

best_results

```

Let's fit the best model, collect the predictions and plot the confusion matrix:

```{r finalize}
#fit the best model
final_fit <- wflow_set_grid_results %>% 
  pull_workflow("rec_rd") %>% 
  finalize_workflow(best_results) %>% 
  last_fit(churn_split) 
```

```{r prediction}
pred<-final_fit %>% collect_predictions()
conf_mat(pred,
         truth = Churn,
         estimate = .pred_class)%>%
  autoplot(type="heatmap")
```
